{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to crawl fraud related data updated on Uwants within 24 hours\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "\n",
    "# 定义搜索关键词\n",
    "keywords = [\"騙徒手法層出不窮\"]\n",
    "\n",
    "# 基础 URL，修改为直接筛选过去24小时内的帖子\n",
    "base_url = 'https://www.uwants.com/search.php?searchsubmit=true&srchtxt={}&srchtime=1d&orderby=most_relevant'\n",
    "\n",
    "# 定义请求头，模拟浏览器请求\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Cache-Control\": \"max-age=0\"\n",
    "}\n",
    "\n",
    "# 定义函数用于提取帖子详情\n",
    "def extract_posts(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.encoding = 'utf-8'\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # 查找帖子容器\n",
    "    posts = soup.find_all(\"tr\")\n",
    "    post_data = []\n",
    "\n",
    "    for post in posts:\n",
    "        title_tag = post.find(\"span\", class_=\"search-result-subject\")\n",
    "        forum_tag = post.find(\"td\", class_=\"search-result-forum\")\n",
    "        author_tag = post.find(\"td\", class_=\"search-result-author\")\n",
    "        reply_view_tag = post.find(\"td\", class_=\"search-result-nums\")\n",
    "        last_post_tag = post.find(\"td\", class_=\"search-result-lastpost\")\n",
    "\n",
    "        if title_tag and forum_tag and author_tag and reply_view_tag and last_post_tag:\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            post_url = title_tag.find(\"a\")[\"href\"]\n",
    "\n",
    "            forum = forum_tag.get_text(strip=True)\n",
    "            author = author_tag.find(\"a\").get_text(strip=True)\n",
    "            reply_view = reply_view_tag.get_text(strip=True)\n",
    "            last_post = last_post_tag.get_text(strip=True)\n",
    "\n",
    "            # 获取帖子内容页\n",
    "            post_content, responses, first_post_time = extract_post_details(post_url)\n",
    "\n",
    "            post_data.append({\n",
    "                \"帖子标题\": title,\n",
    "                \"帖子内容\": post_content,\n",
    "                \"帖子URL\": post_url,\n",
    "                \"版區\": forum,\n",
    "                \"作者\": author,\n",
    "                \"回覆/查看\": reply_view,\n",
    "                \"回复内容\": responses,\n",
    "                \"最初發表时间\": first_post_time,  # 使用楼主的发帖时间\n",
    "            })\n",
    "\n",
    "    return post_data\n",
    "\n",
    "# 获取帖子详细内容及回复，同时提取楼主的发帖时间\n",
    "def extract_post_details(post_url):\n",
    "    all_post_content = []\n",
    "    all_responses = []\n",
    "    first_post_time = None  # 楼主发帖时间\n",
    "\n",
    "    try:\n",
    "        response = requests.get(post_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # 找到所有帖子\n",
    "        posts = soup.find_all('table', class_='threadpost')\n",
    "\n",
    "        for i, post in enumerate(posts):\n",
    "            post_info = post.find('div', class_='postinfo')\n",
    "            post_content_div = post.find('div', class_='postmessage defaultpost')\n",
    "\n",
    "            # 提取发帖时间\n",
    "            if post_info and \"發表於\" in post_info.get_text():\n",
    "                post_time = post_info.get_text(strip=True).split(\"發表於\")[1].strip()\n",
    "            else:\n",
    "                post_time = None\n",
    "\n",
    "            # 提取帖子内容\n",
    "            if post_content_div:\n",
    "                span = post_content_div.find('span', id=lambda x: x and x.startswith('postorig_'))\n",
    "                if span:\n",
    "                    # 删除引用内容\n",
    "                    for quote in span.find_all('div', class_='quote'):\n",
    "                        quote.decompose()\n",
    "                    content = span.get_text(strip=True)\n",
    "                else:\n",
    "                    content = \"\"\n",
    "\n",
    "            # 判断是否为楼主（第一条帖子）\n",
    "            if i == 0:\n",
    "                first_post_time = post_time  # 记录楼主发帖时间\n",
    "                all_post_content.append(content)\n",
    "            else:\n",
    "                all_responses.append(content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching post details from {post_url}: {e}\")\n",
    "\n",
    "    return \" | \".join(all_post_content), \" | \".join(all_responses), first_post_time\n",
    "\n",
    "# 打开 CSV 文件准备写入，使用 utf-8-sig 编码，确保 Excel 正确识别\n",
    "output_file = 'D:/uwants_search_results.csv'  # 保存到 D 盘根目录\n",
    "with open(output_file, mode='w', newline='', encoding='utf-8-sig') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"搜索关键词\", \"帖子标题\", \"帖子内容\", \"帖子URL\", \"版區\", \"作者\", \"回覆/查看\", \"回复内容\", \"最初發表时间\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    # 遍历每个关键词\n",
    "    for keyword in keywords:\n",
    "        print(f\"正在搜索关键词: {keyword}\")\n",
    "\n",
    "        encoded_keyword = quote(keyword)\n",
    "\n",
    "        # 构建访问过去24小时内帖子列表的 URL\n",
    "        search_url = base_url.format(encoded_keyword)\n",
    "        print(f\"正在提取过去24小时内的帖子数据...\")\n",
    "\n",
    "        try:\n",
    "            # 提取当前页面的帖子数据\n",
    "            posts_data = extract_posts(search_url)\n",
    "\n",
    "            # 写入每个帖子的相关信息到 CSV 文件\n",
    "            for post in posts_data:\n",
    "                writer.writerow({\n",
    "                    \"搜索关键词\": keyword,\n",
    "                    \"帖子标题\": post[\"帖子标题\"],\n",
    "                    \"帖子内容\": post[\"帖子内容\"],\n",
    "                    \"帖子URL\": post[\"帖子URL\"],\n",
    "                    \"版區\": post[\"版區\"],\n",
    "                    \"作者\": post[\"作者\"],\n",
    "                    \"回覆/查看\": post[\"回覆/查看\"],\n",
    "                    \"回复内容\": post[\"回复内容\"],\n",
    "                    \"最初發表时间\": post[\"最初發表时间\"]  # 使用楼主的发帖时间\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"提取帖子时发生错误: {e}\")\n",
    "\n",
    "        time.sleep(10)  # 设置爬取间隔，避免被封禁\n",
    "\n",
    "print(\"所有关键词的搜索结果已提取并保存至 'uwants_search_results.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
